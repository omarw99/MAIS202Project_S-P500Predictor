# -*- coding: utf-8 -*-
"""Deliverable2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tHfr2baG0CJJcN1amEGFyr2zAhSZ9kwa
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import csv
import random
import string
import re

from sklearn import svm
from sklearn.svm import LinearSVC
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical

from gensim.models.fasttext import FastText

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import sent_tokenize
from nltk import WordPunctTokenizer

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')
en_stop = set(nltk.corpus.stopwords.words('english'))

articles_datapath = "https://raw.githubusercontent.com/omarw99/MAIS202Project_SP500Predictor/master/Dataset/Combined_News_DJIA.csv"
articles_df = pd.read_csv(articles_datapath).drop(columns = ["Label"])

'''
sLength = len(articles_df['Date'])
articles_df["Combined Headlines"] = pd.Series(np.zeros(sLength), index=articles_df.index)

vector = []
for i in range (len(articles_df)):
  for j in range(1, 26):
    vector.append(articles_df.iloc[i, j])
  print(vector)
  articles_df.iloc[i][26] = vector

print(articles_df)
'''
stockReturn_datapath = "https://raw.githubusercontent.com/omarw99/MAIS202Project_SP500Predictor/master/Dataset/SP500.csv"
stockReturn_df = pd.read_csv(stockReturn_datapath).drop(columns = ["High", "Low", "Close", "Volume"])
#Add column with daily return calculated using adjusted closing and opening prices
stockReturn_df["Daily Return"] = (stockReturn_df["Adj Close"] - stockReturn_df["Open"]) / stockReturn_df["Open"]

combined_df = articles_df.merge(stockReturn_df).drop(columns = ["Open", "Adj Close"])

#Drop any row that has missing data
combined_df = combined_df.dropna()

#Remove stopwords
stop = stopwords.words('english')

for i in range(1, 26):
  combined_df.iloc[:, i] = combined_df.iloc[:, i].apply(lambda x: " ".join(x for x in x.split() if x not in stop))

stemmer = WordNetLemmatizer()

def cleanHeadline(headline):
  #Remove all the special characters
  headline = re.sub(r'\W', ' ', str(headline))

  #Remove all single characters
  headline = re.sub(r'\s+[a-zA-Z]\s+', ' ', headline)

  #Remove single characters from the start
  headline = re.sub(r'\^[a-zA-Z]\s+', ' ', headline)

  #Substituting multiple spaces with single space
  headline = re.sub(r'\s+', ' ', headline, flags=re.I)

  #Removing prefixed 'b'
  headline = re.sub(r'^b\s+', '', headline)

  #Converting to Lowercase
  headline = headline.lower()

  #Remove all punctuation and numbers
  headline = headline.translate(str.maketrans('','',string.punctuation)).translate(str.maketrans('','','1234567890'))

  #Lemmatization
  tokens = headline.split()
  tokens = [stemmer.lemmatize(word) for word in tokens]
  tokens = [word for word in tokens if word not in en_stop]
  tokens = [word for word in tokens if len(word) > 2]

  cleanString = ' '.join(tokens)

  return cleanString

#Call cleanHeadline on all the headlines in the dataset
for i in range(len(combined_df)):
  for j in range(1, 26):
    combined_df.iat[i, j] = cleanHeadline(combined_df.iat[i, j])

#Function that adds a column that has all the words from that day's headlines in a list
def addTokenizedWordsColumn(combined_df):
  combined_df["List of Words"] = np.zeros
  arrayOfDailyHeadlineWords = []
  for i in range(len(combined_df)):
    vector = []
    for j in range(1, 26):
      token = combined_df.iat[i, j].split()
      for k in range(len(token)):
        vector.append(token[k])
    combined_df.iat[i, 27] = vector
    arrayOfDailyHeadlineWords.append(vector)

addTokenizedWordsColumn(combined_df)

#Function that adds a column that has that day's headlines in a single string
def addStringHeadlinesColumn(combined_df):
  combined_df["Combined Headlines"] = np.zeros
  for i in range(len(combined_df)):
    vector = []
    for j in range(1, 26):
      vector.append(combined_df.iat[i, j])
    string = ' '.join(vector)
    combined_df.iat[i, 28] = string

addStringHeadlinesColumn(combined_df)

#Create a series of all the words in the headlines
all_words = [word for i in combined_df["List of Words"] for word in i]

#Create a series of the headline lengths
sentence_lengths = [len(tokens) for tokens in combined_df["List of Words"]]

#Sort all_words and save as a list, get rid of the duplicates to find vocabulary
vocab = sorted(list(set(all_words)))

print("%s words total, with a vocabulary size of %s" % (len(all_words), len(vocab)))
print("Max sentence length is %s" % max(sentence_lengths))

"""DONE WITH DATA PROCESSING AT THIS POINT - 
START VECTORIZING, SHUFFLING, AND DIVIDING UP THE DATASET
"""

#Create a corpus list of all the headline strings from the dataset
list_corpus = combined_df["Combined Headlines"].tolist()
#Create a list of the daily returns
list_labels = combined_df["Daily Return"].tolist()

#Vectorize all the headlines using CountVectorizer() based on list_corpus
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(list_corpus)
vocabulary = vectorizer.get_feature_names()

vectorizedXData = X.toarray()
vectorizedYData = np.asarray(list_labels)

#Create a function that vectorizes a single headline string based on the vocabulary list
def preprocess_sample_point(headline, vocab):
  #Create a vector of the same length as vocab and initialize with all zeroes
  vector = np.zeros((len(vocab),), dtype = int)
  #Clean up headline
  headline = cleanHeadline(headline)
  #Split headline by its words into a list
  wordsInHeadline = headline.split()

  #A double for loop that will check each word in some_string and put a 1 in vector if that word exists in vocab
  #The 1 will get put in the same index as where that word was in vocab
  for i in wordsInHeadline:
    for j in range(len(vocab)):
      if vocab[j] == i:
        vector[j] += 1
  
  return vector

#Test function
print(preprocess_sample_point("Russian Putin president", vocabulary))
print(sum(preprocess_sample_point("Russian Putin president", vocabulary)))

#Shuffle the order of the X and y arrays the same way to maintain their relationship
indices = np.arange(vectorizedXData.shape[0])
np.random.shuffle(indices)

vectorizedXData = vectorizedXData[indices]
vectorizedYData = vectorizedYData[indices]

#Create a function that will split the data into train and test sets
def train_test_split(X, y, train_size):
  trainSetSampleSize = round(train_size * len(X))

  X_train = X[0 : trainSetSampleSize]
  X_test = X[trainSetSampleSize : ]
  y_train= y[0 : trainSetSampleSize]
  y_test= y[trainSetSampleSize : ]
  
  return X_train, X_test, y_train, y_test

#Split data into train, and test sets
percentageInTrainSet = 0.7
#percentageInValidationSet = 0.1
#percentageInTestSet = 1 - percentageInTrainSet - percentageInValidationSet

X_train, X_test, y_train, y_test = train_test_split(vectorizedXData, vectorizedYData, percentageInTrainSet)
#amountInTestSet = 1 - percentageInTrainSet

#X_validate, X_test, y_validate, y_test = train_test_split(X_test, y_test, (percentageInValidationSet/amountInTestSet))

#Check if data was split up correctly
print(X_train.shape)
print(X_test.shape)

print(y_train.shape)
print(y_test.shape)

"""DONE WITH SPLITTING UP THE DATA - START TESTING DIFFERENT MODELS

MODEL #1: RandomForestRegressor
"""

#Call the RandomForestRegressor model and fit using X_train and y_train
regr = RandomForestRegressor(max_depth = 2, random_state = 0)
regr.fit(X_train, y_train)

#Use .predict() to create the y_train_predict and y_test_predict arrays
y_train_predict = regr.predict(X_train)
y_test_predict = regr.predict(X_test)

#Find the MSE of the training and testing sets
mse_train = np.mean(np.square(np.subtract(y_train, y_train_predict)))
print("Training set Mean Squared Error: {}".format(mse_train))

mse_test = np.mean(np.square(np.subtract(y_test, y_test_predict)))
print("Testing set Mean Squared Error: {}".format(mse_test))

#Test model on a single news headline
vector = preprocess_sample_point('Russia declares war on the United States', vocabulary)
print(sum(vector))
vector = vector.reshape(1, -1)
print(regr.predict(vector))

vector1 = preprocess_sample_point('The stock market is booming', vocabulary)
print(sum(vector1))
vector1 = vector.reshape(1, -1)
print(regr.predict(vector1))

"""MODEL #2: SVR"""

#Scale the X_train and X_test matrices so the SVR model works best
scaler = StandardScaler()
scaler.fit(X_train)  # Don't cheat - fit only on training data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)  # apply same transformation to test data

#Call the SVR model and fit using X_train and y_train
clf = svm.SVR()
clf.fit(X_train, y_train)

#Use .predict() to create the y_train_predict and y_test_predict arrays
y_train_predict = clf.predict(X_train)
y_test_predict = clf.predict(X_test)

#Find the MSE of the training and testing sets
mse_train = np.mean(np.square(np.subtract(y_train, y_train_predict)))
print("Training set Mean Squared Error: {}".format(mse_train))

mse_test = np.mean(np.square(np.subtract(y_test, y_test_predict)))
print("Testing set Mean Squared Error: {}".format(mse_test))

#Test model on a single news headline
vector = preprocess_sample_point('Russia declares war on the United States', vocabulary)
print(sum(vector))
vector = vector.reshape(1, -1)
print(clf.predict(vector))

vector1 = preprocess_sample_point('The stock market is booming', vocabulary)
print(sum(vector1))
vector1 = vector.reshape(1, -1)
print(clf.predict(vector1))

"""FASTTEXT MODEL ATTEMPT"""

'''
embedding_size = 60
window_size = 40
min_word = 5
down_sampling = 1e-2

ft_model = FastText(word_tokenized_corpus, size=embedding_size, window=window_size, min_count=min_word, sample=down_sampling, sg=1, iter=100)
'''

'''
vectorSimilarWords = []
for i in range(5):
  vectorSimilarWords.append(allWords[i])
  for j in range(5):
    vectorSimilarWords.append(ft_model.wv.most_similar(allWords[i], topn=5)[j][0])
'''

'''
print(vectorSimilarWords)
print(type(vectorSimilarWords))
print(len(vectorSimilarWords))
'''

'''
word_vectors = ft_model.wv[vectorSimilarWords]

pca = PCA(n_components=2)

p_comps = pca.fit_transform(word_vectors)
word_names = vectorSimilarWords

plt.figure(figsize=(18, 10))
plt.scatter(p_comps[:, 0], p_comps[:, 1], c='red')

for word_names, x, y in zip(word_names, p_comps[:, 0], p_comps[:, 1]):
    plt.annotate(word_names, xy=(x+0.06, y+0.03), xytext=(0, 0), textcoords='offset points')
'''